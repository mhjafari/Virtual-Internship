{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f5e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "import gzip\n",
    "import sys\n",
    "import pickle\n",
    "import keras\n",
    "import math\n",
    "import os\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230d6345",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_data_mvs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-208e8e521954>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;31m# load training and testing data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_IDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-208e8e521954>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, categoryIDs, img_rows, img_cols)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategoryIDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryIDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryIDs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__organize_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__organize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategoryIDs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-208e8e521954>\u001b[0m in \u001b[0;36m__load_labels\u001b[1;34m(self, categoryIDs)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m#data_files = [r\"data/train_data_mvs.txt\", r\"data/test_data_mvs.txt\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_data_mvs.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "import glob, os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "class DataLoader:\n",
    "    num_images_1 = 49 # number of images in type 1 sets (1-77)\n",
    "    num_images_2 = 64 # number of images in type 2 sets (82-128)\n",
    "    input_shape = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    input_shape = []\n",
    "    # accepts the name of a directory and the name of a .npy file as strings\n",
    "    # loads data from the given .npy if it exists, otherwise loads data from\n",
    "    # raw images and saves it to a .npy file for future runs\n",
    "    # returns a numpy array representation of\n",
    "    # the image set in the given directiory\n",
    "    def __load_images(self, directories, img_rows, img_cols):\n",
    "        image_set = []\n",
    "        cwd = os.getcwd() # save current working directory\n",
    "        for directory_num in directories:\n",
    "            new_images = []\n",
    "\n",
    "            os.chdir(r\"C:\\Users\\sulai\\VIRTUAL INTERNSHIP\\Virtual internship\\Cleaned 9 scans\\scan%s\" % directory_num) # switch to directory for image files\n",
    "            os.chdir(r\"D:\\Cleaned\\scan%s\" % directory_num) # switch to directory for image files\n",
    "            if os.path.isfile(\"path%s.npy\" % directory_num):\n",
    "                new_images = np.load(\"path%s.npy\" % directory_num)\n",
    "            else:\n",
    "                for file in glob.glob(\"*max.png\"): # only loads the 'max' image from each view\n",
    "                    img = image.load_img(file, target_size=(img_rows, img_cols))\n",
    "                    img_array = image.img_to_array(img)\n",
    "                    #print(\"array\", directory_num, img_array.shape)\n",
    "                    # do some other preprocessing here?\n",
    "                    new_images.append(img_array)\n",
    "                np.save(\"path%s.npy\" % directory_num, new_images);\n",
    "\n",
    "            if not np.array(image_set).size:\n",
    "                image_set = new_images\n",
    "            else:\n",
    "                image_set= np.concatenate((image_set, new_images), 0)\n",
    "        os.chdir(cwd) # switch back to previous working directory\n",
    "        image_set = np.array(image_set)\n",
    "        #preprocess input\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            image_set = image_set.reshape(image_set.shape[0], 3, img_rows, img_cols)\n",
    "            self.input_shape = (3, img_rows, img_cols)\n",
    "        else:\n",
    "            image_set = image_set.reshape(image_set.shape[0], img_rows, img_cols, 3)\n",
    "            self.input_shape = (img_rows, img_cols, 3)\n",
    "        image_set = image_set.astype('float32')\n",
    "        image_set /= 255\n",
    "        return image_set\n",
    "    # accepts an array of categoryIDs as a parameter\n",
    "    # loads ground relative pose data for images in those categories\n",
    "    def __load_labels(self, categoryIDs):\n",
    "        labels = []\n",
    "        data_files = [r\"data/train_data_mvs.txt\", r\"data/test_data_mvs.txt\"]\n",
    "        #data_files = [r\"data/train_data_mvs.txt\", r\"data/test_data_mvs.txt\"]\n",
    "        for file in data_files:\n",
    "            f = open(file)\n",
    "            for line in f:\n",
    "                if line[0].isdigit():\n",
    "                    sl = line.split()\n",
    "                    nextTuple = (int(sl[0]), int(sl[1]), int(sl[2]),\n",
    "                                 float(sl[3]), float(sl[4]), float(sl[5]),\n",
    "                                 float(sl[6]), float(sl[7]), float(sl[8]), float(sl[9]))\n",
    "                    if nextTuple[2] in categoryIDs:\n",
    "                        labels.append(nextTuple);\n",
    "            f.close()\n",
    "        return np.array(labels)\n",
    "    # accepts array of labels with image identifiers\n",
    "    # returns shuffled labels split into training and testing sets\n",
    "    def __organize_labels(self, labels):\n",
    "        np.random.shuffle(labels)\n",
    "        num_labels = np.array(labels).shape[0]\n",
    "        train_index = int(0.8*num_labels)\n",
    "        train_labels = labels[:train_index,:]\n",
    "        test_labels = labels[train_index:,:]\n",
    "        np.savetxt('index_spp.txt', train_labels, delimiter=' ')\n",
    "        return train_labels, test_labels\n",
    "\n",
    "    # accepts arrays of training and testing labels, \n",
    "    # array of images, and array of category IDs\n",
    "    # returns arrays of image tuples representing the training and\n",
    "    # testing datasets\n",
    "    def __organize_data(self, train_labels, test_labels, images, categoryIDs):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        for label in train_labels:\n",
    "            if label[2] <= 77: # currently can't category IDs above 77\n",
    "                mult = int(categoryIDs.index(label[2]))\n",
    "                img1_index = (mult * self.num_images_1) + int(label[0]) - 1\n",
    "                img2_index = (mult * self.num_images_1) + int(label[1]) - 1\n",
    "                image_tuple = (images[img1_index], images[img2_index])\n",
    "                train_data.append(image_tuple)\n",
    "        for label in test_labels:\n",
    "            if label[2] <= 77: # currently can't category IDs above 77\n",
    "                mult = int(categoryIDs.index(label[2]))\n",
    "                img1_index = (mult * self.num_images_1) + int(label[0]) - 1\n",
    "                img2_index = (mult * self.num_images_1) + int(label[1]) - 1\n",
    "                image_tuple = (images[img1_index], images[img2_index])\n",
    "                test_data.append(image_tuple)\n",
    "        return np.array(train_data), np.array(test_data)\n",
    "    # accepts arrays of training and testing labels\n",
    "    # returs same arrays with image identifying data removed\n",
    "    # final arrays have form: [relative translation, relative orientation]\n",
    "    #\t\t\t\t\t\t  [[x, y, z] [q1, q2, q3, q4]]\n",
    "    def __clean_labels(self, train_labels, test_labels):\n",
    "        return train_labels[:,3:], test_labels[:,3:]\n",
    "    # returns shuffled training and test data consisting of\n",
    "    # lists of image pairs with indicies [pair_num, image_num, width, height, depth]\n",
    "    def get_data(self):\n",
    "        return self.train_data, self.test_data\n",
    "    # returns shuffled training and test labels with form:\n",
    "    # [x, y, z, q1, q2, q3, q4]\n",
    "    def get_labels(self):\n",
    "        return self.train_labels, self.test_labels\n",
    "    def get_input_shape(self):\n",
    "        return self.input_shape\n",
    "    def __init__(self, categoryIDs, img_rows, img_cols):\n",
    "        images = self.__load_images(categoryIDs, img_rows, img_cols)\n",
    "        labels = self.__load_labels(categoryIDs)\n",
    "        self.train_labels, self.test_labels = self.__organize_labels(labels)\n",
    "        self.train_data, self.test_data = self.__organize_data(self.train_labels, self.test_labels, images, categoryIDs)\n",
    "        self.train_labels, self.test_labels = self.__clean_labels(self.train_labels, self.test_labels)\n",
    "\n",
    "\n",
    "\n",
    "beta = 10\n",
    "epochs = 10\n",
    "def custom_objective(y_true, y_pred):\n",
    "    error = K.square(y_pred - y_true)\n",
    "    transMag = K.sqrt(error[0] + error[1] + error[2])\n",
    "    orientMag = K.sqrt(error[3] + error[4] + error[5] + error[6])\n",
    "    return K.mean(transMag + (beta * orientMag))\n",
    "def dot_product(v1, v2):\n",
    "    return sum((a*b) for a,b in zip(v1,v2))\n",
    "def length(v):\n",
    "    return math.sqrt(dot_product(v,v))\n",
    "def compute_mean_error(y_true, y_pred):\n",
    "    trans_error = 0\n",
    "    orient_error = 0\n",
    "    for i in range(0,y_true.shape[0]):\n",
    "        trans_error += math.acos(dot_product(y_true[i,:3], y_pred[i,:3])/\n",
    "                                 (length(y_true[i,:3]) * length(y_pred[i,:3])))\n",
    "        orient_error += math.acos(dot_product(y_true[i,3:], y_pred[i,3:])/\n",
    "                                  (length(y_true[i,3:]) * length(y_pred[i,3:])))\n",
    "    mean_trans = trans_error / y_true.shape[0]\n",
    "    mean_orient = orient_error / y_true.shape[0]\n",
    "    return mean_trans, mean_orient\n",
    "def create_conv_branch(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, kernel_size=(11,11),\n",
    "                     strides=4, padding='valid',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "    model.add(Conv2D(256, kernel_size=(5,5),\n",
    "                     strides=1, padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=1))\n",
    "    model.add(Conv2D(384, kernel_size=(3,3),\n",
    "                     strides=1, padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(384, kernel_size=(3,3),\n",
    "                     strides=1, padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(256, kernel_size=(3,3),\n",
    "                     strides=1, padding='same',\n",
    "                     activation='relu'))\n",
    "    # replace with SPP if possible\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=2))\n",
    "    return model\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    img_rows, img_cols = 227, 227\n",
    "    category_IDs = [6] # category IDs from which to pull test and training data\n",
    "    model_name = 'huge_model_10epoch.h5'\n",
    "    model = None\n",
    "    # load training and testing data:\n",
    "    loader = DataLoader(category_IDs, img_rows, img_cols)\n",
    "    train_data, test_data = loader.get_data()\n",
    "    train_labels, test_labels = loader.get_labels()\n",
    "    input_shape = loader.get_input_shape()\n",
    "    # define structure of convolutional branches\n",
    "    conv_branch = create_conv_branch(input_shape)\n",
    "    branch_a = Input(shape=input_shape)\n",
    "    branch_b = Input(shape=input_shape)\n",
    "    processed_a = conv_branch(branch_a)\n",
    "    processed_b = conv_branch(branch_b)\n",
    "    # compute distance between outputs of the CNN branches\n",
    "    # not sure if euclidean distance is right here\n",
    "    # merging or concatenating inputs may be more accurate\n",
    "    #distance = Lambda(euclidean_distance, \n",
    "    #\t\t\t\t  output_shape = eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    regression = keras.layers.concatenate([processed_a, processed_b])\n",
    "    regression = Flatten()(regression) # may not be necessary\n",
    "    output = Dense(7, kernel_initializer='normal', name='output')(regression)\n",
    "    model = Model(inputs=[branch_a, branch_b], outputs=[output])\n",
    "    model.compile(loss=custom_objective, \n",
    "                  optimizer=keras.optimizers.Adam(lr=.0001, decay=.00001),\n",
    "                  metrics=['accuracy'])\n",
    "    if os.path.isfile(model_name):\n",
    "        print(\"model\", model_name, \"found\")\n",
    "        model.load_weights(model_name)\n",
    "        print(\"model loaded from file\")\n",
    "    else:\n",
    "\n",
    "        model.fit([train_data[:,0], train_data[:,1]], train_labels,\n",
    "                  batch_size=128,\n",
    "                  epochs = epochs,\n",
    "                  validation_split=0.1,\n",
    "                  shuffle=True)\n",
    "        model.save_weights(model_name)\n",
    "        print(\"model saved as file\", model_name)\n",
    "    #pred = model.predict([train_data[:,0], train_data[:,1]])\n",
    "    #train_trans, train_orient = compute_mean_error(pred, train_labels)\n",
    "    pred = model.predict([test_data[:,0], test_data[:,1]])\n",
    "    test_trans, test_orient = compute_mean_error(pred, test_labels)\n",
    "    np.savetxt('pred.txt', pred, delimiter=' ')\n",
    "    np.savetxt('labels.txt', test_labels, delimiter=' ')\n",
    "    #print('* Mean translation error on training set: %0.2f' % (train_trans))\n",
    "    #print('* Mean orientation error on training set: %0.2f' % (train_orient))\n",
    "    print('*     Mean translation error on test set: %0.2f' % (test_trans))\n",
    "    print('*     Mean orientation error on test set: %0.2f' % (test_orient))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
